# Deploy a model to an endpoint
You can integrate a language model with a chat application by deploying the model to an **endpoint**. An endpoint is a specific URL where a deployed model or service can be accessed. Each model deployment typically has its own unique endpoint, which allows different applications to communicate with the model through an **API** (**Application Programming Interface**).
## Deploy a language model with Azure AI Foundry

When you deploy a language model with Azure AI Foundry, you have several types available, which depend on the model you want to deploy.

Deploy options include:

- **Standard deployment**: Models are hosted in the Azure AI Foundry project resource.
- **Serverless compute**: Models are hosted in Microsoft-managed dedicated serverless endpoints in an Azure AI Foundry hub project.
- **Managed compute**: Models are hosted in managed virtual machine images in an Azure AI Foundry hub project.

| Standard deployment | Serverless compute | Managed compute |
| --- | --- | --- |
| Supported models | Azure AI Foundry models (including Azure OpenAI models and Models-as-a-service models) | Foundry Models with pay-as-you-go billing | Open and custom models |
| Hosting service | Azure AI Foundry resource | AI Project resource in a hub | AI Project resource in a hub |
| Billing basis | Token-based billing | Token-based billing | Compute-based billing |
